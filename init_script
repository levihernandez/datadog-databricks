%python 

dbutils.fs.put("dbfs:/tmp/datadog-install-driver-workers.sh","""
#!/bin/bash

# compiled from https://gist.github.com/ChristineTChen/dd6eecaa111579f9be27316b8b49b159 & https://docs.databricks.com/clusters/clusters-manage.html

cat <<EOF >> /tmp/start_datadog.sh
#!/bin/bash
  
  # Declaring the agent outside the if \${DB_IS_DRIVER} helps us deploy the agent on all hosts

  # install the Datadog agent on driver & workers
  DD_AGENT_MAJOR_VERSION=7 DD_API_KEY=${DD_API_KEY} DD_SITE="datadoghq.com" bash -c "\$(curl -L https://s3.amazonaws.com/dd-agent/scripts/install_script.sh)"
  
  sudo sed -i 's/^# env: <environment name>\$/env: ${DD_ENV}/g' /etc/datadog-agent/datadog.yaml
  ddline=\$(sudo sed -n  '\|^# tags:\$|=' /etc/datadog-agent/datadog.yaml)
  ddnum=\$((ddline + 3))

  hostip=$(hostname -I | xargs)
if [[ \${DB_IS_DRIVER} = "TRUE" ]]; then

  echo "Installing Datadog agent in the driver (master node) ..."
  
  # Install the latest Spark structured streaming metrics
  sudo -u dd-agent -- datadog-agent integration install  datadog-spark==1.19.0

  # WRITING SPARK CONFIG FILE FOR STREAMING SPARK METRICS
  echo "init_config:
instances:
    - spark_url: http://\${DB_DRIVER_IP}:\${SPARK_UI_PORT}
      spark_cluster_mode: spark_driver_mode
      cluster_name: \${hostip}
      streaming_metrics: true" > /etc/datadog-agent/conf.d/spark.d/conf.yaml
  
  sudo sed -i '/# tags:/ s/^/tags:\\n  - environment:${DD_ENVIRONMENT}\\n  - cluster_id:${DB_CLUSTER_ID}\\n  - cluster_name:${DB_CLUSTER_NAME}\\n  - host_ip:${SPARK_LOCAL_IP}\\n  - spark_host:driver\\n/'  /etc/datadog-agent/datadog.yaml
else
  sudo sed -i '/# tags:/ s/^/tags:\\n  - environment:${DD_ENVIRONMENT}\\n  - cluster_id:${DB_CLUSTER_ID}\\n  - cluster_name:${DB_CLUSTER_NAME}\\n  - host_ip:${SPARK_LOCAL_IP}\\n  - spark_host:worker\\n/'  /etc/datadog-agent/datadog.yaml
fi
  # RESTARTING AGENT
  sudo service datadog-agent restart
EOF

# CLEANING UP
chmod a+x /tmp/start_datadog.sh
/tmp/start_datadog.sh >> /tmp/datadog_start.log 2>&1 & disown
""", True)
